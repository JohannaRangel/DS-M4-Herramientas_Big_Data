{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/kzhangkzhang/258d18858889fa97194011a249b74c43\n",
    "\n",
    "https://youtu.be/4Dko5W96WHg?si=NO8dWSxu5mhmZjeP\n",
    "\n",
    "df -h #espacio\n",
    "sudo docker ps #ver todos los contenedores\n",
    "sudo docker ps -a #ver container inactivos\n",
    "sudo docker stop $(sudo docker ps -a -q) #detiene todos los contendores\n",
    "sudo docker container prune\n",
    "sudo docker-compose -f docker-compose-v4.yml up -d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  git clone https://github.com/soyHenry/DS-M4-Herramientas_Big_Data.git\n",
    "  cd DS-M4-Herramientas_Big_Data\n",
    "  sudo docker-compose -f docker-compose-v1.yml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFS: Configuración de sistema de archivos distribuido para almacenar datos en HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it namenode bash\n",
    "cd home\n",
    "mkdir Datasets\n",
    "cd Datasets\n",
    "ls #verifico que esten los archivos\n",
    "exit\n",
    "pwd # /home/ubuntu/DS-M4-Herramientas_Big_Data\n",
    "sudo docker cp <path><archivo> namenode:/home/Datasets/<archivo>\n",
    "# sudo docker cp /home/ubuntu/DS-M4-Herramientas_Big_Data namenode:/home/\n",
    "mv /home/Datasets/Datasets/* /home/Datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  sudo docker exec -it namenode bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  hdfs dfs -mkdir -p /data\n",
    "  hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  hdfs dfs -put /home/Datasets/* /data\n",
    "  hdfs dfs -ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " http://<IP_Anfitrion>:9870/conf\n",
    " http://192.168.1.26:9870/\n",
    "\n",
    "#valores de tamaño de bloque \n",
    " #dfs.blocksize\n",
    " #134217728\n",
    "\n",
    "#valores de factor de réplica\n",
    " #dfs.replication\n",
    " #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive: Crear tablas en Hive, a partir de los csv ingestados en HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd DS-M4-Herramientas_Big_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker-compose -f docker-compose-v2.yml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#desde ubuntu\n",
    "sudo docker cp /home/ubuntu/DS-M4-Herramientas_Big_Data/Paso02.hql hive-server:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it hive-server bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#me ubico en la raiza del container que fue donde guardo el archivo (salgo de opt cd..)\n",
    "hive -f /Paso02.hql\n",
    "\n",
    "hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "SHOW DATABASES;\n",
    "USE integrador;\n",
    "SHOW TABLES;\n",
    "DROP DATABASE integrador CASCADE;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatos de Almacenamiento: Las tablas creadas en HIVE, deben ser almacenadas en formato Parquet + Snappy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp /home/ubuntu/DS-M4-Herramientas_Big_Data/Parquet hive-server:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it hive-server \n",
    "cd.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  hdfs dfs -mkdir -p /data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " hdfs dfs -put /Parquet/Datasets2/. /data2\n",
    "\n",
    "\n",
    " hdfs dfs -mv /data2/Datasets2/* /data2/\n",
    " hdfs dfs -ls /data2\n",
    " hdfs dfs -rm -r  /data2/Datasets2/\n",
    " exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp /home/ubuntu/DS-M4-Herramientas_Big_Data/Paso03.hql hive-server:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it hive-server bash\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#no funciono con el scrip Paso03.hql, se crearon las tablas manualmente\n",
    "#archivo: creacion_y_de_las_tablas_punto_3_sin_script\n",
    "\n",
    "#hive -f /Paso03.hql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL: Crear índices en alguna de las tablas cargadas y probar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No-SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "df -h #para verificar el espacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd DS-M4-Herramientas_Big_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker-compose -f docker-compose-v3-2.yml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it hbase-master hbase shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\t\tcreate 'personal','personal_data'\n",
    "\t\tlist 'personal'\n",
    "\t\tput 'personal',1,'personal_data:name','Juan'\n",
    "\t\tput 'personal',1,'personal_data:city','Córdoba'\n",
    "\t\tput 'personal',1,'personal_data:age','25'\n",
    "\t\tput 'personal',2,'personal_data:name','Franco'\n",
    "\t\tput 'personal',2,'personal_data:city','Lima'\n",
    "\t\tput 'personal',2,'personal_data:age','32'\n",
    "\t\tput 'personal',3,'personal_data:name','Ivan'\n",
    "\t\tput 'personal',3,'personal_data:age','34'\n",
    "\t\tput 'personal',4,'personal_data:name','Eliecer'\n",
    "\t\tput 'personal',4,'personal_data:city','Caracas'\n",
    "\t\tget 'personal','4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " sudo docker exec -it namenode bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " hdfs dfs -put /home/Datasets/personal.csv /hbase/data/personal.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it hbase-master bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=',' -Dimporttsv.columns=HBASE_ROW_KEY,personal_data:name,personal_data:city,personal_data:age personal hdfs://namenode:9000/hbase/data/personal.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "    hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=',' -Dimporttsv.columns=HBASE_ROW_KEY,personal_data:name,personal_data:city,personal_data:age personal hdfs://namenode:9000/hbase/data/personal.csv\n",
    "\t\thbase shell\n",
    "\t\tscan 'personal'\n",
    "\t\tcreate 'album','label','image'\n",
    "\t\tput 'album','label1','label:size','10'\n",
    "\t\tput 'album','label1','label:color','255:255:255'\n",
    "\t\tput 'album','label1','label:text','Family album'\n",
    "\t\tput 'album','label1','image:name','holiday'\n",
    "\t\tput 'album','label1','image:source','/tmp/pic1.jpg'\n",
    "\t\tget 'album','label1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd DS-M4-Herramientas_Big_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp /home/ubuntu/DS-M4-Herramientas_Big_Data/Datasets/iris.csv mongodb:/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp /home/ubuntu/DS-M4-Herramientas_Big_Data/Datasets/iris.json mongodb:/data/iris.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it mongodb bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mongoimport /data/iris.csv --type csv --headerline -d dataprueba -c iris_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mongoimport --db dataprueba --collection iris_json --file /data/iris.json --jsonArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mongosh\n",
    "\t\tuse dataprueba\n",
    "\t\tshow collections\n",
    "\t\tdb.iris_csv.find()\n",
    "\t\tdb.iris_json.find()\n",
    "\t\texit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mongoexport --db dataprueba --collection iris_csv --fields sepal_length,sepal_width,petal_length,petal_width,species --type=csv --out /data/iris_export.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mongoexport --db dataprueba --collection iris_json --fields sepal_length,sepal_width,petal_length,petal_width,species --type=json --out /data/iris_export.json\n",
    "exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Desde cd DS-M4-Herramientas_Big_Data/Mongo\n",
    "sudo docker cp mongo-hadoop-hive-2.0.2.jar namenode:/tmp/udfs/mongo-hadoop-hive-2.0.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp mongo-hadoop-core-2.0.2.jar namenode:/tmp/udfs/mongo-hadoop-core-2.0.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp mongo-hadoop-spark-2.0.2.jar namenode:/tmp/udfs/mongo-hadoop-spark-2.0.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker cp mongo-java-driver-3.12.11.jar namenode:/tmp/udfs/mongo-java-driver-3.12.11.jar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Desde cd DS-M4-Herramientas_Big_Data/\n",
    "sudo docker cp iris.hql hive-server:/opt/iris.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it namenode  bash\n",
    "cd /tmp/udfs #verificamos que existan los archivos\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir -p /tmp/udfs\n",
    "hdfs dfs -ls /tmp/udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " hdfs dfs -put /tmp/udfs/* /tmp/udfs\n",
    " hdfs dfs -ls /tmp/udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it hive-server bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hiveserver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "chmod 777 iris.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "hive -f iris.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker exec -it neo4j bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cypher-shell\n",
    "usuario : neo4j\n",
    "contraseña :zeppelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "val flights = spark.read.format(\"csv\").option(\"sep\", \",\").option(\"header\", \"true\").load(\"hdfs://namenode:9000/data/raw-flight-data.csv\").as[flightSchema]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker-compose up -d\n",
    "\t\t\tsudo docker exec -it kafka_container bash\n",
    "\t\t\tcd /opt/kafka/bin\n",
    "\t\t\tsh kafka-topics.sh --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 100 --topic demo\n",
    "\t\t\tsh kafka-topics.sh --list --bootstrap-server kafka:9092\n",
    "\t\t\tsh kafka-topics.sh --describe --bootstrap-server kafka:9092 --topic demo \n",
    "\t\t\tsh kafka-console-producer.sh --broker-list localhost:9092 --topic demo\n",
    "\t\t\t\tEscribir desde la consola del productor \"Esto es una Prueba 1\" y enviar.\n",
    "            #Ejecutamos desde otra consola putty\n",
    "            sh kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic demo --from-beginning\n",
    "\t\t\t\t"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
